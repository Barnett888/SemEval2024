# -*- coding: utf-8 -*-
"""llama.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VNZsJt2aEHOIXjx0cUNzggsAzJudoJeG
"""

import json
#!pip install accelerate==0.26.0
#!pip install datasets
#!pip install torch
from datasets import Dataset
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import AutoModelForCausalLM
from transformers import DataCollatorWithPadding
from transformers import TrainingArguments
from transformers import Trainer
import numpy as np
import torch
import pandas as pd
import evaluate

"""First read the file. Please place the two data set and this python file into a same folder."""

with open("subtaskA_train_monolingual.jsonl", 'r', encoding='utf-8') as f:
    training_input=(f.read())
with open("subtaskA_monolingual.jsonl", 'r', encoding='utf-8') as f:
    testing_input=(f.read())

#from google.colab import drive
#drive.mount('/content/drive')

"""Set the text and label into two lists, then transfer them to a dataset."""

training_input=training_input.split("\n")
training_input = [json.loads(training_input[i])for i in range(len(training_input)) if len(training_input[i])>0]
training_text = [training_input[i]["text"]for i in range(len(training_input))]
training_label = [training_input[i]["label"]for i in range(len(training_input))]
training_data = {"text": training_text,"label": training_label}
training_data = Dataset.from_dict(training_data)
testing_input=testing_input.split("\n")
testing_input = [json.loads(testing_input[i])for i in range(len(testing_input)) if len(testing_input[i])>0]
testing_text = [testing_input[i]["text"]for i in range(len(testing_input))]
testing_label = [testing_input[i]["label"]for i in range(len(testing_input))]
testing_data = {"text": testing_text,"label": testing_label}
testing_data = Dataset.from_dict(testing_data)

"""Tokenize the text input."""

#!huggingface-cli login

tokenizer = AutoTokenizer.from_pretrained("NousResearch/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("NousResearch/Llama-2-7b-chat-hf")
def tokenize_function(dataset):
    return tokenizer(dataset["text"], padding="max_length", truncation=True)
tokenized_training = training_data.map(tokenize_function, batched=True)
tokenized_testing = testing_data.map(tokenize_function, batched=True)
del training_input
del testing_input
del training_data
del training_text
del testing_text

"""Set up the pre-trained model."""

#!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    # create Trainer
training_args = TrainingArguments(
  #output_dir="/content/drive/MyDrive/ipynb/nlp_a2/llama_checkpoint.txt",
  output_dir="/llama_checkpoint.txt",
  learning_rate=2e-5,
  per_device_train_batch_size=4,
  per_device_eval_batch_size=4,
  num_train_epochs=3,
  weight_decay=0.01,
  evaluation_strategy="epoch",
  save_strategy="epoch",
  load_best_model_at_end=True,
)
def compute_metrics(eval_pred):

    f1_metric = evaluate.load("f1")

    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    results = {}
    results.update(f1_metric.compute(predictions=predictions, references = labels, average="micro"))

    return results
trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=tokenized_training,
  eval_dataset=tokenized_testing,
  tokenizer=tokenizer,
  data_collator=data_collator,
  compute_metrics=compute_metrics
)

"""Train the model."""

#!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

trainer.train()
model.save_pretrained("/llama_model")

"""The downstream work can start here to save time. Use the model to produce predictions"""

model = AutoModelForSequenceClassification.from_pretrained("/llama_model")
predictions = trainer.predict(tokenized_testing)
preds = np.argmax(predictions.predictions, axis=-1)

"""Save the labels for the downstream work."""

pre=preds.tolist()
with open("/llama_label.jsonl", 'w', encoding='utf-8') as f:
  for i in range(len(pre)):
    json.dump(pre[i], f)
    f.write('\n')

"""The downstream work can start here to save time. Read the predictions from the trained model."""

with open("/llama_label.jsonl", 'r', encoding='utf-8') as f:
    result=(f.read())
result=[result]
result=result[0].split("\n")
label=[]
id=[]
for i in range(len(result)-1):
  label.append(int(result[i]))
  id.append(i)

"""Generate the output of the required format."""

predictions_df = pd.DataFrame({'id': id, 'label': label})
predictions_df.to_json("/llama.jsonl", lines=True, orient='records')