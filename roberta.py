# -*- coding: utf-8 -*-
"""RoBERTa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PyNniRlwg3h5ekG_mKaVxGE95zmyTvFh
"""

import json
!pip install accelerate==0.26.0
!pip install datasets
!pip install evaluate
from datasets import Dataset
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import DataCollatorWithPadding
from transformers import TrainingArguments
from transformers import Trainer
import numpy as np
import evaluate

"""First read the file."""

with open("/content/drive/MyDrive/ipynb/nlp_a2/subtaskA_train_monolingual.jsonl", 'r', encoding='utf-8') as f:
    training_input=(f.read())
with open("/content/drive/MyDrive/ipynb/nlp_a2/subtaskA_monolingual.jsonl", 'r', encoding='utf-8') as f:
    testing_input=(f.read())

"""Set the text and label into two lists, then transfer them to a dataset."""

training_input=training_input.split("\n")
training_input = [json.loads(training_input[i])for i in range(len(training_input)) if len(training_input[i])>0]
training_text = [training_input[i]["text"]for i in range(len(training_input))]
training_label = [training_input[i]["label"]for i in range(len(training_input))]
training_data = {"text": training_text,"label": training_label}
training_data = Dataset.from_dict(training_data)
testing_input=testing_input.split("\n")
testing_input = [json.loads(testing_input[i])for i in range(len(testing_input)) if len(testing_input[i])>0]
testing_text = [testing_input[i]["text"]for i in range(len(testing_input))]
testing_label = [testing_input[i]["label"]for i in range(len(testing_input))]
testing_data = {"text": testing_text,"label": testing_label}
testing_data = Dataset.from_dict(testing_data)

"""Tokenize the text input."""

!huggingface-cli login

tokenizer = AutoTokenizer.from_pretrained("roberta-base")
def tokenize_function(dataset):
    return tokenizer(dataset["text"], padding="max_length", truncation=True)
tokenized_training = training_data.map(tokenize_function, batched=True)
tokenized_testing = testing_data.map(tokenize_function, batched=True)
del training_input
del testing_input
del training_data
del training_text
del testing_text

"""Set up the pre-trained model."""

model = AutoModelForSequenceClassification.from_pretrained("roberta-base")
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    # create Trainer
training_args = TrainingArguments(
  output_dir="/content/drive/MyDrive/ipynb/nlp_a2/bert_checkpoint.txt",
  learning_rate=2e-5,
  per_device_train_batch_size=16,
  per_device_eval_batch_size=16,
  num_train_epochs=3,
  weight_decay=0.01,
  evaluation_strategy="epoch",
  save_strategy="epoch",
  load_best_model_at_end=True,
)
def compute_metrics(eval_pred):

    f1_metric = evaluate.load("f1")

    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    results = {}
    results.update(f1_metric.compute(predictions=predictions, references = labels, average="micro"))

    return results
trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=tokenized_training,
  eval_dataset=tokenized_testing,
  tokenizer=tokenizer,
  data_collator=data_collator,
  compute_metrics=compute_metrics
)

"""Train the model."""

trainer.train()

"""Use the model to produce predictions"""

model.save_pretrained("/content/drive/MyDrive/ipynb/nlp_a2/roberta_model")

predictions = trainer.predict(tokenized_testing)
preds = np.argmax(predictions.predictions, axis=-1)

"""Save the labels for the downstream work."""

pre=preds.tolist()
with open("/content/drive/MyDrive/ipynb/nlp_a2/roberta_label.jsonl", 'w', encoding='utf-8') as f:
  for i in range(len(pre)):
    json.dump(pre[i], f)
    f.write('\n')

trainer.save_model("/content/drive/MyDrive/ipynb/nlp_a2/")

"""The downstream work can start here to save time."""

!pip install datasets
from datasets import Dataset
import pandas as pd

"""Read the predictions from the trained model."""

with open("/content/drive/MyDrive/ipynb/nlp_a2/roberta_label.jsonl", 'r', encoding='utf-8') as f:
    result=(f.read())
result=[result]
result=result[0].split("\n")
label=[]
id=[]
for i in range(len(result)-1):
  label.append(int(result[i]))
  id.append(i)

"""Generate the output of the requiered format."""

predictions_df = pd.DataFrame({'id': id, 'label': label})
predictions_df.to_json("/content/drive/MyDrive/ipynb/nlp_a2/roberta.jsonl", lines=True, orient='records')