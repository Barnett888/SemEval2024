# SemEval2024

Conribution: Equal Contribution

How to run: Each model script generates a .jsonl file that can be directly used to formatchecker.py and scorer.py. 
            The scorer.py should print out the accuracy, micro and macro F1 score. See original task github to see shell 
            command

SemEval 2024 Task 8: https://github.com/mbzuai-nlp/SemEval2024-task8 Multidomain, Multimodel and Multilingual Machine-Generated Text Detection. classify texts into having been generated by a Large Language Model (LLM) or not.

Subtask A: Binary Human-Written vs. Machine-Generated Text Classification: Given a full text, determine whether it is human-written or machine-generated. Use only the data for subtask A: monolingual, that is only the English texts. Please ignore the multilingual data.

Download the training data from the Google drive link for Task A, https://drive.google.com/drive/folders/1CAbb3DjrOPBNm0ozVBfhvrEh9P9rAppcthe file subtaskA_train_monolingual.jsonl  
(there is a dev data file there, you can use it or not, it is optional). 
Subtask A (monolingual) contains 119,757 texts for training and 5,000 as dev data, with labels, in JSON format, for each object:

{
  id -> identifier of the example,
  label -> label (human text: 0, machine text: 1,),
  text -> text generated by a machine or written by a human,
  model -> model that generated the data,
  source -> source (Wikipedia, Wikihow, Peerread, Reddit, Arxiv)  on English or language (Arabic, Russian, Chinese, Indonesian, Urdu, Bulgarian, German)
}

 
The test data is available in the file subtaskA_monolingual.jsonl At https://drive.google.com/drive/folders/10DKtClzkwIIAatzHBWXZXuQNID-DNGSG as well as the expected solution (gold standard) in a file with the same name
https://drive.google.com/drive/folders/13aFJK4UyY3Gxg_2ceEAWfJvzopB1vkPc

System produce a prediction file, one single JSONL file for all the texts in the test data. The entry for each text must include the fields "id" and "label".

There is a format checker if you want to verify that your prediction file complies with the expected format. 
 
The main evaluation metric is accuracy. However, the scorer also reports macro-F1 and micro-F1. The scorer is run by a command line with two files as parameters 

python3 subtaskA/scorer/scorer.py --gold_file_path=<path_to_gold_labels> --pred_file_path=<path_to_your_results_file>

The output of the system classifier is a single .json file
